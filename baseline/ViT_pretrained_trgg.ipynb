{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bcf617f3-83e5-4156-84bd-4afc8ec89201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "import pickle\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import random_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3bbd9af4-246a-4e88-9296-dc079c40b035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxmin1(x):\n",
    " \n",
    "    max_value = x.max().item()\n",
    "    print ('1:', max_value)\n",
    "    min_value = x.min().item()  \n",
    "    print ('2:', min_value)\n",
    "    x_normalized = (x - min_value) / (max_value - min_value)\n",
    "     \n",
    "    return x_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "674e940b-e5df-408c-a6b8-53d3150073fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(RG_GRFf_file_path, RG_GRFl_file_path, abn_ratio):\n",
    "\n",
    "    with open(RG_GRFf_file_path, 'rb') as file:\n",
    "        GRFf = pickle.load(file).astype(np.float32)\n",
    "    file.close()\n",
    "\n",
    "    with open(RG_GRFl_file_path, 'rb') as file:\n",
    "        GRFl = pickle.load(file).astype(np.float32)\n",
    "    file.close()\n",
    "    \n",
    "    # stat indice for 0(abnormal)/1(healthy)\n",
    "    GRFl_0_indice = np.where(GRFl==0)\n",
    "    GRFl_1_indice = np.where(GRFl==1)\n",
    "    \n",
    "    # print ('6:', GRFl_0_indice)\n",
    "    # print ('7:', GRFl_1_indice)\n",
    "    \n",
    "    # select the corresponding features for indice\n",
    "    GRFl_features_0 = GRFf[list(GRFl_0_indice[0])]\n",
    "    GRFl_features_1 = GRFf[list(GRFl_1_indice[0])]\n",
    "    \n",
    "    # select the corresponding features for indice\n",
    "    GRFl_0 = GRFl[list(GRFl_0_indice[0])]\n",
    "    GRFl_1 = GRFl[list(GRFl_1_indice[0])]\n",
    "\n",
    "    # print (GRFl_features_0.shape)\n",
    "    \n",
    "    # print (GRFl_features_1.shape)\n",
    "    \n",
    "    # 67977 abnoraml / 16574 healthy, 因此至少\n",
    "    # print (round(len(GRFl_0)/len(GRFl_1),3))\n",
    "    \n",
    "    # print (GRFl_0.shape)\n",
    "    \n",
    "    # print (GRFl_1.shape)\n",
    "\n",
    "    # 训练样本\n",
    "    #### Portation of  0(abnormal)\n",
    "    total_sample_size = GRFl_features_0.shape[0]\n",
    "    total_sample_range = list(np.arange(0, total_sample_size))\n",
    "    train_sample_size = int(GRFl_features_0.shape[0] * abn_ratio)\n",
    "    # 训练\n",
    "    train_indices = np.random.choice(GRFl_features_0.shape[0], size = train_sample_size, replace=False)\n",
    "    # print ('1:', indices)\n",
    "    # print ('2:', type(indices))\n",
    "    # print ('3:', indices.shape)\n",
    "    GRFl_features_0_portion_train = GRFl_features_0[train_indices]\n",
    "    GRFl_0_portion_train = GRFl_0[train_indices]\n",
    "    # print ('4:', GRFl_features_0_portion)\n",
    "    # print ('5:', type(GRFl_features_0_portion))\n",
    "    # print ('6:', GRFl_features_0_portion.shape)\n",
    "    # print ('44:', GRFl_0_portion)\n",
    "    # print ('7:', type(GRFl_0_portion))\n",
    "    # print ('8:', GRFl_0_portion.shape)    \n",
    "    #### Concatenate of  0(abnormal) + 1(healthy)\n",
    "    ## 3398 + 16574 = 19972\n",
    "    GRFf_train = np.concatenate((GRFl_features_0_portion_train, GRFl_features_1),axis = 0)\n",
    "    GRFl_train = np.concatenate((GRFl_0_portion_train, GRFl_1),axis = 0)\n",
    "    # print ('9:', type(GRFf)) \n",
    "    # print ('10:', GRFf.shape) \n",
    "    # print ('11:', type(GRFl)) \n",
    "    # print ('12:', GRFl.shape)\n",
    "    \n",
    "    # 测试样本\n",
    "    test_indices = []\n",
    "    for e in total_sample_range:\n",
    "        if e not in train_indices:\n",
    "            # test_indices.append(np.where(total_sample_range==e)[0][0])\n",
    "            test_indices.append(e)\n",
    "            \n",
    "    ##\n",
    "    test_indices = np.array(test_indices)\n",
    "    GRFl_features_0_portion_test = GRFl_features_0[test_indices]\n",
    "    GRFl_0_portion_test = GRFl_0[test_indices]\n",
    "    ##\n",
    "    GRFf_test = np.concatenate((GRFl_features_0_portion_test, GRFl_features_1), axis = 0)\n",
    "    GRFl_test = np.concatenate((GRFl_0_portion_test, GRFl_1), axis = 0)\n",
    "\n",
    "    ####\n",
    "    print ('13:', GRFf_train.shape)\n",
    "    print ('14:', GRFl_train.shape)\n",
    "    print ('15:', GRFf_test.shape)\n",
    "    print ('16:', GRFl_test.shape)\n",
    "\n",
    "    return GRFf_train, GRFl_train, GRFf_test, GRFl_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dddaf21e-5527-4a98-a436-a5fa39fb32fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_01(X, Y):\n",
    "    GRFf = X\n",
    "    GRFl = Y\n",
    "    # 使用SMOTE进行过采样时正样本和负样本要放在一起，生成比例1：1\n",
    "    smo = SMOTE(n_jobs=-1)\n",
    "    # 这里必须是fit_resample()，有些版本是fit_sample()无法运行\n",
    "    #### reshape, 用KNN来插值维度必须是1D\n",
    "    GRFf_1d = GRFf.reshape(GRFf.shape[0], -1)\n",
    "    # print ('test1:', GRFf.shape)\n",
    "    # 特征维度\n",
    "    GRFf_1d_fnum = GRFf.shape[1]\n",
    "    \n",
    "    # print ('1:', GRFf_1d.shape)\n",
    "    GRFf_re, GRFl_re = smo.fit_resample(GRFf_1d, GRFl)\n",
    "    \n",
    "    # print ('2:', GRFf_re.shape)\n",
    "    # print ('3:', GRFl_re.shape)\n",
    "    # 样本\n",
    "    GRFf_re = GRFf_re.reshape(GRFf_re.shape[0], GRFf_1d_fnum, -1)\n",
    "    #\n",
    "    # print ('4:', GRFf_re.shape)\n",
    "    \n",
    "    \n",
    "    #### test the distribution proportion\n",
    "    #\n",
    "    #\n",
    "    GRFl_re_0_indice = np.where(GRFl_re==0)\n",
    "    GRFl_re_1_indice = np.where(GRFl_re==1)\n",
    "    \n",
    "    #### 1:1，少数类样本0(疾病人群)数量增加为16574(0.05)\n",
    "    # print ('5:', len(list(GRFl_re_0_indice[0]))) \n",
    "    # print ('6:', len(list(GRFl_re_1_indice[0]))) \n",
    "    # print ('7:', GRFf_re)\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    print ('17:', len(list(GRFl_re_0_indice[0])))\n",
    "    print ('18:', len(list(GRFl_re_1_indice[0])))    \n",
    "    \n",
    "    return GRFf_re, GRFl_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7d087d-6c85-4502-812b-65e830c640e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b30c31e-da8e-478a-99cc-66fc13edc252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d591af-8208-42f2-9538-82fc81313bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DataSet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a28f62af-04ec-414e-9694-e398421e3953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13: (36967, 10, 101)\n",
      "14: (36967,)\n",
      "15: (64158, 10, 101)\n",
      "16: (64158,)\n",
      "17: 20393\n",
      "18: 20393\n",
      "17: 47584\n",
      "18: 47584\n",
      "1: 1.6962571144104004\n",
      "2: -0.43713119626045227\n",
      "1: 1.6862719058990479\n",
      "2: -0.43713119626045227\n"
     ]
    }
   ],
   "source": [
    "# ---------------1、load---------------\n",
    "RG_GRFf_file_path = 'DataSet1/GRFf.pkl'\n",
    "RG_GRFl_file_path = 'DataSet1/GRFl.pkl'\n",
    "\n",
    "# part ratio，这里健康样本是陪衬，一直要加上\n",
    "abn_ratio = 0.3 # 30%训练\n",
    "# abn_ratio = 0.2 # 20%训练\n",
    "# abn_ratio = 0.1 # 10%\n",
    "# abn_ratio = 0.05 # 5%\n",
    "\n",
    "\n",
    "# 按比例切分样本，但目前是固定10%:90%, 20%:80%, 30%:70%, 但目前的10% 20% 30%必须变起来写成随机化的(sampler???)\n",
    "GRFf_train_part, GRFl_train_part, GRFf_val_part, GRFl_val_part  =  partition(RG_GRFf_file_path, RG_GRFl_file_path, abn_ratio)\n",
    "# 平衡训练集样本量，正负样本数为1:1\n",
    "GRFf_train_ba, GRFl_train_ba = balance_01(GRFf_train_part, GRFl_train_part)\n",
    "# 平衡训练集样本量，正负样本数为1:1\n",
    "GRFf_val_ba, GRFl_val_ba = balance_01(GRFf_val_part, GRFl_val_part)\n",
    "\n",
    "# ------------------------ 2、normlization max-min scaler------------------------\n",
    "GRFf_train = torch.from_numpy(GRFf_train_ba)\n",
    "GRFf_train = maxmin1(GRFf_train)\n",
    "\n",
    "GRFf_val = torch.from_numpy(GRFf_val_ba)\n",
    "GRFf_val = maxmin1(GRFf_val)\n",
    "\n",
    "#\n",
    "GRFl_train = torch.from_numpy(GRFl_train_ba)\n",
    "GRFl_val = torch.from_numpy(GRFl_val_ba)\n",
    "\n",
    "# ------------------------ 3、Tensor Dataset------------------------\n",
    "train_set = TensorDataset(GRFf_train, GRFl_train)\n",
    "val_set = TensorDataset(GRFf_val, GRFl_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "544ce7a9-7095-4bff-b6f6-361d74e95df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11: <torch.utils.data.dataloader.DataLoader object at 0x00000222771E8450>\n",
      "12: <torch.utils.data.dataloader.DataLoader object at 0x000002218E59F450>\n",
      "13: 319\n",
      "14: 2974\n"
     ]
    }
   ],
   "source": [
    "# -----------------4、dataloader------------------------\n",
    "abn_ratio = 0.01 # 1% \n",
    "val_percent = 0.3\n",
    "# train_batch_size = 32\n",
    "train_batch_size = 128\n",
    "\n",
    "# test_batch_size = 48\n",
    "test_batch_size = 32\n",
    "\n",
    "workers  = 3\n",
    "pin_memory = True\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size = train_batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = workers,\n",
    "    pin_memory = pin_memory,\n",
    "    sampler = None\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_set,\n",
    "    batch_size = test_batch_size,\n",
    "    shuffle = False,\n",
    "    num_workers = workers,\n",
    "    pin_memory = pin_memory\n",
    ")\n",
    "\n",
    "print ('11:', train_loader)\n",
    "print ('12:', val_loader)\n",
    "\n",
    "print ('13:', len(train_loader))\n",
    "print ('14:', len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cc1ae9-1d26-452e-bdfb-8585953ae7fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9c819a-3018-4350-87b7-5bdeb19e2113",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ViT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8c70cd4-2315-42f6-9ead-74f0e9fbd4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1、 Linear Projection of Flattened Patches\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    2D Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    # 输入图片大小，patch大小，输入图片channel，token大小（16×16×3）\n",
    "    def __init__(self, img_size=224, patch_size=16, in_c=3, embed_dim=768, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = (img_size, img_size)\n",
    "        patch_size = (patch_size, patch_size)\n",
    "        self.img_size = img_size\n",
    "        # self.patch_size = patch_size\n",
    "        # self.patch_size = 3\n",
    "        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "        # self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.num_patches = 101\n",
    "\n",
    " # RuntimeError: Expected 4-dimensional input for 4-dimensional weight [768, 10, 16, 16], but got 3-dimensional input of size [32, 10, 101] instead\n",
    "\n",
    "        self.proj = nn.Conv1d(in_c, embed_dim, kernel_size=3, stride=1, padding=1)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    " \n",
    "    def forward(self, x):\n",
    "        # B, C, H, W = x.shape\n",
    "        # 如果传入图片的高和宽和预设不同会报错\n",
    "        # assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "        #     f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        B,C,L = x.shape \n",
    "        # flatten: [B, C, H, W] -> [B, C, HW]\n",
    "        # transpose: [B, C, HW] -> [B, HW, C]\n",
    "        # # 128 × 768 × 101 → 128 × 101 × 768(x)\n",
    "        x = self.proj(x).transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 2、 Attention Module\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim,  # 输入token的dim\n",
    "                 num_heads=8,  # 几头\n",
    "                 qkv_bias=False,\n",
    "                 qk_scale=None,\n",
    "                 attn_drop_ratio=0.,\n",
    "                 proj_drop_ratio=0.):\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        # 得到q，k，v之后，根据num_heads对q，k，v进行切块，切块后的dim就是dim/头数\n",
    "        head_dim = dim // num_heads\n",
    "        # 就是根号d，等于q，k的维度\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        # 直接通过一个全连接层生成q，k，v\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop_ratio)\n",
    "        # 表示映射矩阵Wo\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop_ratio)\n",
    " \n",
    "    def forward(self, x):\n",
    "        # [batch_size, num_patches + 1, total_embed_dim]\n",
    "        B, N, C = x.shape\n",
    " \n",
    "        # qkv(): -> [batch_size, num_patches + 1, 3 * total_embed_dim]\n",
    "        # reshape: -> [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head]\n",
    "        # permute: -> [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        # [batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    " \n",
    "        # transpose: -> [batch_size, num_heads, embed_dim_per_head, num_patches + 1]\n",
    "        # @: multiply -> [batch_size, num_heads, num_patches + 1, num_patches + 1]\n",
    "        # @是矩阵乘法，*scale就是是乘根号d\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    " \n",
    "        # @: multiply -> [batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        # transpose: -> [batch_size, num_patches + 1, num_heads, embed_dim_per_head]\n",
    "        # reshape: -> [batch_size, num_patches + 1, total_embed_dim]\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        # *Wo\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# 3、 MLP Block\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# 4、 Transformer Block\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=4.,  # 第一个Linear输出倍数\n",
    "                 qkv_bias=False,\n",
    "                 qk_scale=None,\n",
    "                 drop_ratio=0.,\n",
    "                 attn_drop_ratio=0.,\n",
    "                 drop_path_ratio=0.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm):\n",
    "        super(Block, self).__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                              attn_drop_ratio=attn_drop_ratio, proj_drop_ratio=drop_ratio)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path_ratio) if drop_path_ratio > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop_ratio)\n",
    " \n",
    "    def forward(self, x):\n",
    "        # Res1\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 5、 Vision Transformer network\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_c=10, num_classes=1,\n",
    "                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, qkv_bias=True,\n",
    "                 qk_scale=None, representation_size=None, distilled=False, drop_ratio=0.,\n",
    "                 attn_drop_ratio=0., drop_path_ratio=0., embed_layer=PatchEmbed, norm_layer=None,\n",
    "                 act_layer=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int, tuple): input image size\n",
    "            patch_size (int, tuple): patch size\n",
    "            in_c (int): 输入通道数\n",
    "            num_classes (int): 多分类数量\n",
    "            embed_dim (int): embedding后维度\n",
    "            depth (int): Encoder数量\n",
    "            num_heads (int): multi-head头数\n",
    "            mlp_ratio (int): mlp隐藏层维度是输入的多少倍\n",
    "            qkv_bias (bool): qkv的Linear过程有没有偏置？\n",
    "            qk_scale (float): 类似根号dimk\n",
    "            representation_size (Optional[int]): MLP head是否只有一个全连接层，对应Pre-logits，是一个可选项\n",
    "            distilled (bool): 用于Deit的，与Vit无关\n",
    "            drop_ratio (float): dropout rate\n",
    "            attn_drop_ratio (float): attention dropout rate\n",
    "            drop_path_ratio (float): stochastic depth rate\n",
    "            embed_layer (nn.Module): Embedding层\n",
    "            norm_layer: (nn.Module): normalization层\n",
    "        \"\"\"\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.num_classes = num_classes # 1000\n",
    "        self.num_features = self.embed_dim = embed_dim  # 768\n",
    "        self.num_tokens = 2 if distilled else 1 # 不管\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        act_layer = act_layer or nn.GELU\n",
    "        # 224,16,3,768\n",
    "        self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches # 101\n",
    " \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) # 1,1,768\n",
    "        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if distilled else None # 不管\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim)) # 位置编码，注意要+1（cls token）\n",
    "        self.pos_drop = nn.Dropout(p=drop_ratio) # 位置编码后的dropout\n",
    " \n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_ratio, depth)]  # 构建一个dropout的等差序列，但默认为0\n",
    "        # *可以解引用\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                  drop_ratio=drop_ratio, attn_drop_ratio=attn_drop_ratio, drop_path_ratio=dpr[i],\n",
    "                  norm_layer=norm_layer, act_layer=act_layer)\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    " \n",
    "        # 之前所说的可选项,略过\n",
    "        if representation_size and not distilled:\n",
    "            self.has_logits = True\n",
    "            self.num_features = representation_size\n",
    "            self.pre_logits = nn.Sequential(OrderedDict([\n",
    "                (\"fc\", nn.Linear(embed_dim, representation_size)),\n",
    "                (\"act\", nn.Tanh())\n",
    "            ]))\n",
    "        else:\n",
    "            self.has_logits = False\n",
    "            self.pre_logits = nn.Identity() # 什么也不做\n",
    " \n",
    "        # 分类，不用管else 768 --> 1000\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    " \n",
    "        # 以下与Vit无关 //开始\n",
    "        self.head_dist = None\n",
    "        if distilled:\n",
    "            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()\n",
    "        # 结束//\n",
    " \n",
    "        # 权重初始化\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02) # 位置编码\n",
    "        if self.dist_token is not None:\n",
    "            nn.init.trunc_normal_(self.dist_token, std=0.02)\n",
    " \n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02) # cls token\n",
    "        self.apply(_init_vit_weights)\n",
    " \n",
    "    def forward_features(self, x):\n",
    "        # [B, C, H, W] -> [B, num_patches, embed_dim]\n",
    "        x = self.patch_embed(x)  # [B, 196, 768]\n",
    "        # [1, 1, 768] -> [B, 1, 768]\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        if self.dist_token is None:\n",
    "            x = torch.cat((cls_token, x), dim=1)  # [B, 197, 768]\n",
    "        else:\n",
    "            x = torch.cat((cls_token, self.dist_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    " \n",
    "        x = self.pos_drop(x + self.pos_embed)\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        if self.dist_token is None:\n",
    "            return self.pre_logits(x[:, 0])\n",
    "        else:\n",
    "            return x[:, 0], x[:, 1]\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        if self.head_dist is not None:\n",
    "            x, x_dist = self.head(x[0]), self.head_dist(x[1])\n",
    "            if self.training and not torch.jit.is_scripting():\n",
    "                # during inference, return the average of both classifier predictions\n",
    "                return x, x_dist\n",
    "            else:\n",
    "                return (x + x_dist) / 2\n",
    "        else:\n",
    "            x = self.head(x)\n",
    "        return x\n",
    " \n",
    " \n",
    "def _init_vit_weights(m):\n",
    "    \"\"\"\n",
    "    ViT weight initialization\n",
    "    :param m: module\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.trunc_normal_(m.weight, std=.01)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Conv1d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.zeros_(m.bias)\n",
    "        nn.init.ones_(m.weight)\n",
    " \n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    \"\"\"\n",
    "    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    " \n",
    " \n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"\n",
    "    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    " \n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "    \n",
    "# def vit_base_patch16_224(num_classes: int = 1000):\n",
    "#     \"\"\"\n",
    "#     ViT-Base model (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\n",
    "#     ImageNet-1k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n",
    "#     weights ported from official Google JAX impl:\n",
    "#     链接: https://pan.baidu.com/s/1zqb08naP0RPqqfSXfkB2EA  密码: eu9f\n",
    "#     \"\"\"\n",
    "#     model = VisionTransformer(img_size=224,\n",
    "#                               patch_size=16,\n",
    "#                               embed_dim=768,\n",
    "#                               depth=12,\n",
    "#                               num_heads=12,\n",
    "#                               representation_size=None,\n",
    "#                               num_classes=num_classes)\n",
    "#     return model\n",
    " \n",
    "# vit_base_patch16_224()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c19da96-8ecc-44f9-87f9-cb86278890d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DataSet API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b06d87-4887-4c6f-abbd-9fe9e1edef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8885be3a-5299-4e93-8d05-d3088e2e7528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv1d(10, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (pre_logits): Identity()\n",
      "  (head): Linear(in_features=768, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = VisionTransformer(img_size=224,\n",
    "                          patch_size=16,\n",
    "                          embed_dim=768,\n",
    "                          depth=12,\n",
    "                          num_heads=12,\n",
    "                          representation_size=None,\n",
    "                          num_classes=1)\n",
    "print (net)\n",
    "net=net.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(net.parameters())\n",
    "# 0.1 reduce / 10 epochs\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)# 0.1 reduce / 10 epochs\n",
    "\n",
    "\n",
    "###\n",
    "###\n",
    "# epochs = 100\n",
    "epochs = 3\n",
    "\n",
    "# acc\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    y_pred = y_pred>0.5\n",
    "    y_pred = torch.where(y_pred, 1, 0)\n",
    "    correct = torch.eq(y_true, y_pred).sum().item() \n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "\n",
    "# confusion matrix\n",
    "# 混淆矩阵计算\n",
    "def calculate_performance_metrics(y_true, y_pred, labels):\n",
    "\n",
    "    # 预测矩阵转换0 1矩阵\n",
    "    y_pred = y_pred>0.5\n",
    "    y_pred = torch.where(y_pred,1,0)\n",
    "\n",
    "    # print ('0:', y_pred)\n",
    "    # print ('00:', y_true)\n",
    "    # print ('0_:', y_pred.shape)\n",
    "    # print ('00_:', y_true.shape)\n",
    "    \n",
    "\n",
    "    # 再转换为numpy类型\n",
    "    # y_pred = y_pred.detach().numpy()\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    \n",
    "    # y_true = y_true.detach().numpy()\n",
    "    y_true = y_true.cpu().numpy()\n",
    "\n",
    "    # print ('11:', type(y_pred))\n",
    "    # print ('22:', type(y_true))\n",
    "    # print ('33:', y_pred.shape)\n",
    "    # print ('44:', y_true.shape)\n",
    "\n",
    "    # 计算混淆矩阵\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # 混淆矩阵画图\n",
    "#     fx = plot_cm(cm)\n",
    "    \n",
    "    # 计算精度\n",
    "    precision = precision_score(y_true, y_pred, labels = labels, average = 'macro')\n",
    "    \n",
    "    # 计算召回率\n",
    "    recall = recall_score(y_true, y_pred, labels = labels, average = 'macro')\n",
    "    \n",
    "    # 计算F1分数\n",
    "    f1 = f1_score(y_true, y_pred, labels = labels, average = 'macro')\n",
    "    \n",
    "    # 计算平衡F1分数\n",
    "    # 这里假设有一个函数balanced_accuracy_score来计算平衡F1分数\n",
    "    balanced_f1 = balanced_accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    return cm, precision, recall, f1, balanced_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c128cc4-2f2b-475e-83fd-7ba84240ef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a6435c2e-2bdf-4f40-8550-c85606e061f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time on cpu:0.000 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.100000321865082e-05"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "def print_train_time(start:float,\n",
    "                     end:float,\n",
    "                     device:torch.device =None):\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}:{total_time:.3f} seconds\")\n",
    "    return total_time\n",
    " \n",
    "start_time = timer()\n",
    "# 模型的运算在此处进行……\n",
    "end_time = timer()\n",
    "print_train_time(start=start_time,end=end_time,device=\"cpu\")\n",
    " \n",
    "# 运行结果\n",
    "# Train time on cpu:0.000 seconds\n",
    "# 3.819999983534217e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b19647da-12a1-452a-80a6-17e0d0c55168",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e1f9f2b9-c3cc-4d0b-8930-402cf6149ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0\n",
      "------\n",
      "Look at 0/40786 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|██████████████████████████▋                                                     | 1/3 [33:58<1:07:56, 2038.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss:1.9252 | Test loss:1.9271, Test acc:50.0000\n",
      "avg-precision: 0.25\n",
      "avg-recall: 0.25\n",
      "avg-f1: 0.25\n",
      "avg-balanced_f1: 0.5\n",
      "last-cm: [[47584. 47584.]\n",
      " [95168. 47584.]]\n",
      "Epoch:1\n",
      "------\n",
      "Look at 0/40786 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|██████████████████████████                                                    | 1/3 [1:07:00<2:14:01, 4020.98s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 62\u001b[0m\n\u001b[0;32m     56\u001b[0m val_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mcriterion(val_pred, y_val)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# print ('test_loss:', loss)\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# print ('val_pred:', val_pred)\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# print ('y_val:', y_val)\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# print ('val_pred:', val_pred.shape)\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# print ('y_val:', val_pred.shape)\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m val_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m accuracy_fn(y_true \u001b[38;5;241m=\u001b[39m y_val, y_pred \u001b[38;5;241m=\u001b[39m val_pred)\n\u001b[0;32m     63\u001b[0m cm, precision, recall, f1, balanced_f1 \u001b[38;5;241m=\u001b[39m calculate_performance_metrics(y_true \u001b[38;5;241m=\u001b[39m y_val, y_pred \u001b[38;5;241m=\u001b[39m val_pred, labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     65\u001b[0m precision_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m precision\n",
      "Cell \u001b[1;32mIn[56], line 26\u001b[0m, in \u001b[0;36maccuracy_fn\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     24\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m     25\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(y_pred, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m correct \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meq(y_true, y_pred)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem() \n\u001b[0;32m     27\u001b[0m acc \u001b[38;5;241m=\u001b[39m (correct \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_pred)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m acc\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import *\n",
    "torch.manual_seed(42)\n",
    "train_time_start_on_cpu = timer() \n",
    " \n",
    "##############################################\n",
    "for epoch in tqdm(range(epochs)):  \n",
    "    print(f\"Epoch:{epoch}\\n------\")\n",
    "    train_loss = 0 \n",
    "    for batch,(X,y) in enumerate(train_loader): \n",
    "        #\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # print ('55:', X.shape)\n",
    "        # print ('66:', y.shape)\n",
    "        net.train() \n",
    "        y_pred = net(X).squeeze()\n",
    "        #\n",
    "        # print ('1:', y_pred)\n",
    "        # print ('2:', y_pred.shape)\n",
    "        # print ('3:', y)\n",
    "        # print ('4:', y.shape)        \n",
    " \n",
    "        loss = criterion(y_pred, y)\n",
    "        # print ('train_loss:', loss)\n",
    "        train_loss+=loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    " \n",
    "        loss.backward()\n",
    "        # \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    " \n",
    "        # if batch % 400 == 0:\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Look at {batch * len(X)}/{len(train_loader.dataset)} samples.\")\n",
    "            # break\n",
    " \n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "\n",
    "    ###################val/per epoch###########################\n",
    "    val_loss, val_acc = 0, 0 \n",
    "    precision_t = 0\n",
    "    recall_t = 0\n",
    "    f1_t = 0\n",
    "    balanced_f1_t = 0\n",
    "    cm_t = np.zeros((2,2))\n",
    "    net.eval() \n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for x_val, y_val in val_loader:\n",
    "            #\n",
    "            x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "            val_pred = net(x_val).squeeze() \n",
    "            val_loss+=criterion(val_pred, y_val)\n",
    "            # print ('test_loss:', loss)\n",
    "            # print ('val_pred:', val_pred)\n",
    "            # print ('y_val:', y_val)\n",
    "            # print ('val_pred:', val_pred.shape)\n",
    "            # print ('y_val:', val_pred.shape)\n",
    "            val_acc += accuracy_fn(y_true = y_val, y_pred = val_pred)\n",
    "            cm, precision, recall, f1, balanced_f1 = calculate_performance_metrics(y_true = y_val, y_pred = val_pred, labels=[0,1])\n",
    "            \n",
    "            precision_t += precision\n",
    "            recall_t += recall\n",
    "            f1_t += f1\n",
    "            balanced_f1_t += balanced_f1\n",
    "            cm_t += cm\n",
    "            \n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc /= len(val_loader)\n",
    "        precision_t /=len(val_loader)\n",
    "        recall_t /=len(val_loader)\n",
    "        f1_t /=len(val_loader)\n",
    "        balanced_f1_t /=len(val_loader)\n",
    "        \n",
    "        \n",
    "    \n",
    "    print(f\"\\nTrain loss:{train_loss:.4f} | Test loss:{val_loss:.4f}, Test acc:{val_acc:.4f}\")\n",
    "    \n",
    "    print ('avg-precision:', precision_t)\n",
    "    print ('avg-recall:', recall_t)\n",
    "    print ('avg-f1:', f1_t)\n",
    "    print ('avg-balanced_f1:', balanced_f1_t)\n",
    "    print ('last-cm:', cm_t)\n",
    "\n",
    "\n",
    "# 计算训练时间\n",
    "train_time_end_on_cpu = timer()\n",
    "total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu,\n",
    "                                            end=train_time_end_on_cpu,\n",
    "                                            device=str(next(net.parameters()).device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2499949-de71-41b3-b6aa-69d74ae38616",
   "metadata": {},
   "outputs": [],
   "source": [
    "### vis testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841ab021-91d2-4524-8651-06dc31831a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "### completed testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e624a2e-a582-4f66-8716-55e635a7879e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
